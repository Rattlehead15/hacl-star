include "../../../arch/ppc64le/Vale.PPC64LE.InsBasic.vaf"
include "../../../arch/ppc64le/Vale.PPC64LE.InsMem.vaf"
include "../../../arch/ppc64le/Vale.PPC64LE.InsVector.vaf"
include "../../../arch/ppc64le/Vale.PPC64LE.InsStack.vaf"

include{:fstar}{:open} "FStar.Seq.Base"
include{:fstar}{:open} "FStar.Seq.Properties"

include{:fstar}{:open} "Vale.Def.Words_s"
include{:fstar}{:open} "Vale.Def.Types_s"
include{:fstar}{:open} "Vale.Def.Words.Seq_s"
include{:fstar}{:open} "Vale.Arch.Types"
include{:fstar}{:open} "Spec.SHA2"
include{:fstar}{:open} "Vale.SHA.SHA_helpers"
include{:fstar}{:open} "Spec.Agile.Hash"
include{:fstar}{:open} "Spec.Hash.PadFinish"
include{:fstar}{:open} "Spec.Hash.Definitions"
include{:fstar}{:open} "Vale.PPC64LE.Machine_s"
include{:fstar}{:open} "Vale.PPC64LE.State"
include{:fstar}{:open} "Vale.PPC64LE.Decls"
include{:fstar}{:open} "Vale.PPC64LE.QuickCode"
include{:fstar}{:open} "Spec.Loops"
include{:fstar}{:open} "Vale.SHA2.Wrapper"

module Vale.SHA.PPC64LE

#verbatim{:interface}{:implementation}
open Vale.Def.Opaque_s
open Vale.Def.Types_s
open Vale.Def.Words_s
open Vale.Def.Words.Seq_s
open FStar.Seq
open Vale.Arch.Types
open Vale.Arch.HeapImpl
open Vale.PPC64LE.Machine_s
open Vale.PPC64LE.Memory
open Vale.PPC64LE.Stack_i
open Vale.PPC64LE.State
open Vale.PPC64LE.Decls
open Vale.PPC64LE.QuickCode
open Vale.PPC64LE.QuickCodes
open Vale.PPC64LE.InsBasic
open Vale.PPC64LE.InsMem
open Vale.PPC64LE.InsStack
open Vale.PPC64LE.InsVector
open Vale.SHA.SHA_helpers
open Spec.SHA2
open Spec.Agile.Hash
open Spec.Hash.PadFinish
open Spec.Hash.Definitions
open Spec.Loops
open Vale.SHA2.Wrapper
#reset-options "--z3rlimit 2000"
#endverbatim

procedure Loop_rounds_3_7_11_body(
        inline i:nat,
        out msg:vec_opr,
        ghost in_b:buffer128,
        ghost offset:nat)
    {:quick}
    lets
        inp @= r4;
    reads
        heap0; memLayout;
    modifies
        inp; r10;
    requires        
        0 <= i /\ i < 15 /\ (i % 4) == 3;

        @msg == i + 1;

        validSrcAddrsOffset128(heap0, inp, in_b, offset, 1, memLayout, Secret);
        inp + 16 < pow2_64;
    ensures
        msg == reverse_bytes_quad32(buffer128_read(in_b, offset, heap0));
        inp == old(inp) + 16;
{
    LoadImm64(r10, 0);
    Load128_byte16_buffer(heap0, msg, inp, r10, Secret, in_b, offset);
    AddImm(inp, inp, 16);
}

procedure Loop_rounds_1_15_shift_body(
        inline i:nat,
        out msg0:vec_opr,
        in msg1:vec_opr,
        ghost block:block_w)
    {:quick}
    {:typecheck false}
    requires        
        0 <= i /\ i < 16 /\ (i % 4) != 0;

        @msg0 == i; @msg1 == i - 1;

        i % 4 == 1 ==> msg1.hi3 == ws_opaque(block, i-1) /\ msg1.hi2 == ws_opaque(block, i) /\ msg1.lo1 == ws_opaque(block, i+1) /\ msg1.lo0 == ws_opaque(block, i+2);
        i % 4 == 2 ==> msg1.hi3 == ws_opaque(block, i-1) /\ msg1.hi2 == ws_opaque(block, i) /\ msg1.lo1 == ws_opaque(block, i+1);
        i % 4 == 3 ==> msg1.hi3 == ws_opaque(block, i-1) /\ msg1.hi2 == ws_opaque(block, i);
    ensures
        i % 4 == 1 ==> msg0.hi3 == ws_opaque(block, i) /\ msg0.hi2 == ws_opaque(block, i+1) /\ msg0.lo1 == ws_opaque(block, i+2);
        i % 4 == 2 ==> msg0.hi3 == ws_opaque(block, i) /\ msg0.hi2 == ws_opaque(block, i+1);
        i % 4 == 3 ==> msg0.hi3 == ws_opaque(block, i);
{
    Vsldoi(msg0, msg1, msg1, 4);
}

procedure Loop_rounds_16_63_body(
        inline i:nat,
        inout msg0:vec_opr,
        in msg1:vec_opr,
        in msg2:vec_opr,
        in msg3:vec_opr,
        ghost block:block_w)
    {:quick}
    {:typecheck false}
    lets
        tmp_vec @= v25; tmp_vec2 @= v26;
    modifies
        tmp_vec; tmp_vec2;
    requires        
        16 <= i /\ i < 64;

        let j := i % 16;

        @msg0 == j; @msg1 == (j + 1) % 16; @msg2 == (j + 9) % 16; @msg3 == (j + 14) % 16;

        msg0.hi3 == ws_opaque(block, i-16);
        msg1.hi3 == ws_opaque(block, i-15);
        msg2.hi3 == ws_opaque(block, i-7);
        msg3.hi3 == ws_opaque(block, i-2);
    ensures
        let sigma0 := sigma256_0_0(ws_opaque(block, i-15));
        let sigma1 := sigma256_0_1(ws_opaque(block, i-2));
        msg0.hi3 == add_wrap32(add_wrap32(add_wrap32(ws_opaque(block, i-16), sigma0), sigma1), ws_opaque(block, i-7));
        msg0.hi3 == ws_opaque(block, i);
{
    SHA256_sigma0(tmp_vec, msg1, i, block);
    lemma_sigma_0_0_partial(i, block);
    Vadduwm(msg0, msg0, tmp_vec);
    SHA256_sigma1(tmp_vec2, msg3, i, block);
    lemma_sigma_0_1_partial(i, block);
    Vadduwm(msg0, msg0, tmp_vec2);
    Vadduwm(msg0, msg0, msg2);
    lemma_ws_opaque(block, i);
}

procedure Loop_rounds_0_63_body(
        inline i:nat,
        in msg:vec_opr,
        in a_vec:vec_opr,
        in b_vec:vec_opr,
        in c_vec:vec_opr,
        inout d_vec:vec_opr,
        in e_vec:vec_opr,
        in f_vec:vec_opr,
        inout g_vec:vec_opr,
        inout h_vec:vec_opr,
        ghost k_b:buffer128,
        ghost block:block_w,
        ghost hash_orig:hash256)
    {:quick}
    lets
        Wi @= v24; tmp_vec @= v25; tmp_vec2 @= v26;
    reads
        heap0; Wi;
    modifies
        tmp_vec; tmp_vec2;
    requires
        0 <= i /\ i < 64;

        @msg == i % 16;
        i % 8 == 0 ==> @a_vec == 16 && @b_vec == 17 && @c_vec == 18 && @d_vec == 19 && @e_vec == 20 && @f_vec == 21 && @g_vec == 22 && @h_vec == 23;
        i % 8 == 1 ==> @a_vec == 23 && @b_vec == 16 && @c_vec == 17 && @d_vec == 18 && @e_vec == 19 && @f_vec == 20 && @g_vec == 21 && @h_vec == 22;
        i % 8 == 2 ==> @a_vec == 22 && @b_vec == 23 && @c_vec == 16 && @d_vec == 17 && @e_vec == 18 && @f_vec == 19 && @g_vec == 20 && @h_vec == 21;
        i % 8 == 3 ==> @a_vec == 21 && @b_vec == 22 && @c_vec == 23 && @d_vec == 16 && @e_vec == 17 && @f_vec == 18 && @g_vec == 19 && @h_vec == 20;
        i % 8 == 4 ==> @a_vec == 20 && @b_vec == 21 && @c_vec == 22 && @d_vec == 23 && @e_vec == 16 && @f_vec == 17 && @g_vec == 18 && @h_vec == 19;
        i % 8 == 5 ==> @a_vec == 19 && @b_vec == 20 && @c_vec == 21 && @d_vec == 22 && @e_vec == 23 && @f_vec == 16 && @g_vec == 17 && @h_vec == 18;
        i % 8 == 6 ==> @a_vec == 18 && @b_vec == 19 && @c_vec == 20 && @d_vec == 21 && @e_vec == 22 && @f_vec == 23 && @g_vec == 16 && @h_vec == 17;
        i % 8 == 7 ==> @a_vec == 17 && @b_vec == 18 && @c_vec == 19 && @d_vec == 20 && @e_vec == 21 && @f_vec == 22 && @g_vec == 23 && @h_vec == 16;

        let ks := buffer128_as_seq(heap0, k_b);

        k_reqs(ks);
        
        let hash := #(seq(Vale.SHA.SHA_helpers.word))(repeat_range_vale(i, block, hash_orig));
        a_vec.hi3 == word_to_nat32(index(hash, 0));
        b_vec.hi3 == word_to_nat32(index(hash, 1));
        c_vec.hi3 == word_to_nat32(index(hash, 2));
        d_vec.hi3 == word_to_nat32(index(hash, 3));
        e_vec.hi3 == word_to_nat32(index(hash, 4));
        f_vec.hi3 == word_to_nat32(index(hash, 5));
        g_vec.hi3 == word_to_nat32(index(hash, 6));
        h_vec.hi3 == add_wrap32(word_to_nat32(index(hash, 7)), k_index(ks, i));
        msg.hi3 == ws_opaque(block, i);
        i != 63 ==> Wi.hi3 == k_index(ks, i+1);
    ensures
        let ks := buffer128_as_seq(heap0, k_b);
        let hash := #(seq(Vale.SHA.SHA_helpers.word))(repeat_range_vale(i, block, hash_orig));
        let h_k := add_wrap32(word_to_nat32(index(hash, 7)), k_index(ks, i));
        let ch := ch256(word_to_nat32(index(hash, 4)), word_to_nat32(index(hash, 5)), word_to_nat32(index(hash, 6)));
        let sigma1 := sigma256_1_1(word_to_nat32(index(hash, 4)));
        let sigma0 := sigma256_1_0(word_to_nat32(index(hash, 0)));
        let maj := maj256(word_to_nat32(index(hash, 0)), word_to_nat32(index(hash, 1)), word_to_nat32(index(hash, 2)));
        let sigma0_maj := add_wrap32(sigma0, maj);
        d_vec.hi3 == add_wrap32(word_to_nat32(index(hash, 3)), add_wrap32(add_wrap32(add_wrap32(h_k, ws_opaque(block, i)), ch), sigma1));
        h_vec.hi3 == add_wrap32(add_wrap32(add_wrap32(add_wrap32(h_k, ws_opaque(block, i)), ch), sigma1), sigma0_maj);
        i != 63 ==> g_vec.hi3 == add_wrap32(word_to_nat32(index(hash, 6)), k_index(ks, i+1));
        i != 63 ==> (let next_hash := #(seq(Vale.SHA.SHA_helpers.word))(repeat_range_vale(i+1, block, hash_orig)) in
            a_vec.hi3 == word_to_nat32(index(next_hash, 1)) /\
            b_vec.hi3 == word_to_nat32(index(next_hash, 2)) /\
            c_vec.hi3 == word_to_nat32(index(next_hash, 3)) /\
            d_vec.hi3 == word_to_nat32(index(next_hash, 4)) /\
            e_vec.hi3 == word_to_nat32(index(next_hash, 5)) /\
            f_vec.hi3 == word_to_nat32(index(next_hash, 6)) /\
            g_vec.hi3 == add_wrap32(word_to_nat32(index(next_hash, 7)), k_index(ks, i+1)) /\
            h_vec.hi3 == word_to_nat32(index(next_hash, 0)));
        i == 63 ==> make_seperated_hash_quad32(h_vec, a_vec, b_vec, c_vec, d_vec, e_vec, f_vec, g_vec) == repeat_range_vale_64(block, hash_orig);
{
    Vadduwm(h_vec, h_vec, msg);
    Vsel(tmp_vec, g_vec, f_vec, e_vec);
    lemma_vsel32(old(f_vec.hi3), old(g_vec.hi3), old(e_vec.hi3));
    inline if (i <> 63)
    {
        Vadduwm(g_vec, g_vec, Wi);
    } 
    Vadduwm(h_vec, h_vec, tmp_vec);
    SHA256_Sigma1(tmp_vec2, e_vec, i, block, hash_orig);
    lemma_sigma_1_1_partial(i, block, hash_orig);
    Vadduwm(h_vec, h_vec, tmp_vec2);
    Vxor(tmp_vec, a_vec, b_vec);
    Vsel(tmp_vec, b_vec, c_vec, tmp_vec);
    quad32_xor_reveal();
    lemma_eq_maj_xvsel32(old(a_vec.hi3), old(b_vec.hi3), old(c_vec.hi3));
    Vadduwm(d_vec, d_vec, h_vec);
    SHA256_Sigma0(tmp_vec2, a_vec, i, block, hash_orig);
    lemma_sigma_1_0_partial(i, block, hash_orig);
    Vadduwm(tmp_vec2, tmp_vec2, tmp_vec);
    Vadduwm(h_vec, h_vec, tmp_vec2);
    lemma_shuffle_core_properties(i, block, hash_orig);
    inline if (i = 63)
    {
        lemma_make_seperated_hash(repeat_range_vale_64(block, hash_orig), h_vec, a_vec, b_vec, c_vec, d_vec, e_vec, f_vec, g_vec);
    } 
}

procedure Loop_prologue(ghost in_b:buffer128, ghost offset:nat, ghost k_b:buffer128, ghost block:block_w, ghost hash_orig:hash256)
    {:quick}
    lets
        inp @= r4; tbl @= r6; msg0 @= v0; h_vec @= v23; Wi @= v24;
    reads
        heap0; memLayout;
    modifies
        inp; tbl; r10; msg0; h_vec; Wi;
    requires
        let hash := #(seq(Vale.SHA.SHA_helpers.word))(repeat_range_vale(0, block, hash_orig));
        validSrcAddrsOffset128(heap0, inp, in_b, offset, 1, memLayout, Secret);
        validSrcAddrs128(heap0, tbl, k_b, 1, memLayout, Secret);
        inp + 16 < pow2_64;
        tbl + 16 < pow2_64;
        h_vec.hi3 == word_to_nat32(index(hash, 7));
    ensures
        let ks := buffer128_read(k_b, 0, heap0);
        let hash := #(seq(Vale.SHA.SHA_helpers.word))(repeat_range_vale(0, block, hash_orig));
        msg0 == reverse_bytes_quad32(buffer128_read(in_b, offset, heap0));
        inp == old(inp) + 16;
        tbl == old(tbl) + 16;
        h_vec.hi3 == add_wrap32(word_to_nat32(index(hash, 7)), ks.lo0);
        Wi.hi3 == ks.lo1 && Wi.hi2 == ks.hi2 && Wi.lo1 == ks.hi3;
{
    LoadImm64(r10, 0);
    Load128_byte16_buffer(heap0, msg0, inp, r10, Secret, in_b, offset);
    Load128_word4_buffer(heap0, Wi, tbl, r10, Secret, k_b, 0);
    AddImm(inp, inp, 16);
    AddImm(tbl, tbl, 16);
    Vadduwm(h_vec, h_vec, Wi);
    Vsldoi(Wi, Wi, Wi, 4);
}

procedure Loop_round_0_61_body(inline i:nat, ghost k_b:buffer128)
    {:quick}
    lets
        tbl @= r6; Wi @= v24;
    reads
        heap0; memLayout;
    modifies
        r10; tbl; Wi;
    requires
        0 <= i /\ i < 62;
        let ks := buffer128_read(k_b, (i+2)/4, heap0);
        i%4 == 0 ==> Wi.hi3 == ks.lo1 && Wi.hi2 == ks.hi2 && Wi.lo1 == ks.hi3;
        i%4 == 1 ==> Wi.hi3 == ks.hi2 && Wi.hi2 == ks.hi3;
        i%4 == 2 ==> validSrcAddrsOffset128(heap0, tbl, k_b, (i+2)/4, 1, memLayout, Secret) && tbl + 16 < pow2_64;
        i%4 == 3 ==> Wi.hi3 == ks.lo0 && Wi.hi2 == ks.lo1 && Wi.lo1 == ks.hi2 && Wi.lo0 == ks.hi3;
    ensures
        let ks := buffer128_read(k_b, (i+2)/4, heap0);
        i%4 == 0 ==> Wi.hi3 == ks.hi2 && Wi.hi2 == ks.hi3 && tbl == old(tbl);
        i%4 == 1 ==> Wi.hi3 == ks.hi3 && tbl == old(tbl);
        i%4 == 2 ==> Wi.hi3 == ks.lo0 && Wi.hi2 == ks.lo1 && Wi.lo1 == ks.hi2 && Wi.lo0 == ks.hi3 && tbl == old(tbl) + 16;
        i%4 == 3 ==> Wi.hi3 == ks.lo1 && Wi.hi2 == ks.hi2 && Wi.lo1 == ks.hi3 && tbl == old(tbl);
{
    inline if (i % 4 = 2)
    {
        LoadImm64(r10, 0);
        Load128_word4_buffer(heap0, Wi, tbl, r10, Secret, k_b, (i+2)/4);
        AddImm(tbl, tbl, 16);
    }
    else
    {
        Vsldoi(Wi, Wi, Wi, 4);
    }
}

procedure Loop_rounds_0_15(
        ghost in_b:buffer128,
        ghost offset:nat,
        ghost k_b:buffer128,
        ghost block:block_w,
        ghost hash_orig:hash256,
        ghost input_BE:seq(quad32))
    {:quick}
    lets
        inp @= r4; tbl @= r6;
        a_vec @= v16; b_vec @= v17; c_vec @= v18; d_vec @= v19;
        e_vec @= v20; f_vec @= v21; g_vec @= v22; h_vec @= v23;
        Wi @= v24; tmp_vec @= v25; tmp_vec2 @= v26;
    reads
        heap0; memLayout;
    modifies
        inp; tbl; r10; v0; v1; v2; v3; v4; v5; v6; v7; v8; v9; v10; v11; v12; v13; v14; v15; a_vec; b_vec; c_vec; d_vec; e_vec; f_vec; g_vec; h_vec; Wi; tmp_vec; tmp_vec2;
    requires
        validSrcAddrsOffset128(heap0, inp, in_b, offset+1, 3, memLayout, Secret);
        validSrcAddrsOffset128(heap0, tbl, k_b, 1, 4, memLayout, Secret);
        let ks := buffer128_as_seq(heap0, k_b);
        k_reqs(ks);
        inp + 48 < pow2_64;
        tbl + 64 < pow2_64;
        input_BE == reverse_bytes_quad32_seq(slice(buffer128_as_seq(heap0, in_b), offset, offset+4));
        block == quads_to_block_be(input_BE);
        v0 == index(input_BE, 0);
        let hash := #(seq(Vale.SHA.SHA_helpers.word))(repeat_range_vale(0, block, hash_orig)) in
        a_vec.hi3 == word_to_nat32(index(hash, 0)) /\
        b_vec.hi3 == word_to_nat32(index(hash, 1)) /\
        c_vec.hi3 == word_to_nat32(index(hash, 2)) /\
        d_vec.hi3 == word_to_nat32(index(hash, 3)) /\
        e_vec.hi3 == word_to_nat32(index(hash, 4)) /\
        f_vec.hi3 == word_to_nat32(index(hash, 5)) /\
        g_vec.hi3 == word_to_nat32(index(hash, 6)) /\
        h_vec.hi3 == add_wrap32(word_to_nat32(index(hash, 7)), k_index(ks, 0));
        Wi.hi3 == k_index(ks, 1) && Wi.hi2 == k_index(ks, 2) && Wi.lo1 == k_index(ks, 3);
    ensures
        tbl == old(tbl) + 64;
        inp == old(inp) + 48;
        let ks := buffer128_as_seq(heap0, k_b);
        let next_hash := #(seq(Vale.SHA.SHA_helpers.word))(repeat_range_vale(16, block, hash_orig)) in 
        a_vec.hi3 == word_to_nat32(index(next_hash, 0)) /\
        b_vec.hi3 == word_to_nat32(index(next_hash, 1)) /\
        c_vec.hi3 == word_to_nat32(index(next_hash, 2)) /\
        d_vec.hi3 == word_to_nat32(index(next_hash, 3)) /\
        e_vec.hi3 == word_to_nat32(index(next_hash, 4)) /\
        f_vec.hi3 == word_to_nat32(index(next_hash, 5)) /\
        g_vec.hi3 == word_to_nat32(index(next_hash, 6)) /\
        h_vec.hi3 == add_wrap32(word_to_nat32(index(next_hash, 7)), k_index(ks, 16));
        v0.hi3 == ws_opaque(block, 16) /\ v1.hi3 == ws_opaque(block, 1) /\ v2.hi3 == ws_opaque(block, 2) /\
        v3.hi3 == ws_opaque(block, 3) /\ v4.hi3 == ws_opaque(block, 4) /\ v5.hi3 == ws_opaque(block, 5) /\
        v6.hi3 == ws_opaque(block, 6) /\ v7.hi3 == ws_opaque(block, 7) /\ v8.hi3 == ws_opaque(block, 8) /\
        v9.hi3 == ws_opaque(block, 9) /\ v10.hi3 == ws_opaque(block, 10) /\ v11.hi3 == ws_opaque(block, 11) /\
        v12.hi3 == ws_opaque(block, 12) /\ v13.hi3 == ws_opaque(block, 13) /\ v14.hi3 == ws_opaque(block, 14) /\
        v15.hi3 == ws_opaque(block, 15);
        Wi.hi3 == k_index(ks, 17) && Wi.hi2 == k_index(ks, 18) && Wi.lo1 == k_index(ks, 19);
{
    lemma_quads_to_block_be(input_BE);

    Loop_rounds_0_63_body(0, v0, a_vec, b_vec, c_vec, d_vec, e_vec, f_vec, g_vec, h_vec, k_b, block, hash_orig);
    Loop_round_0_61_body(0, k_b);
    Loop_rounds_1_15_shift_body(1, v1, v0, block);
    Loop_rounds_0_63_body(1, v1, h_vec, a_vec, b_vec, c_vec, d_vec, e_vec, f_vec, g_vec, k_b, block, hash_orig);
    Loop_round_0_61_body(1, k_b);
    Loop_rounds_1_15_shift_body(2, v2, v1, block);
    Loop_rounds_0_63_body(2, v2, g_vec, h_vec, a_vec, b_vec, c_vec, d_vec, e_vec, f_vec, k_b, block, hash_orig);
    Loop_round_0_61_body(2, k_b);

    Loop_rounds_3_7_11_body(3, v4, in_b, offset+1);
    assert v4 == index(input_BE, 1);
    Loop_rounds_1_15_shift_body(3, v3, v2, block);
    Loop_rounds_0_63_body(3, v3, f_vec, g_vec, h_vec, a_vec, b_vec, c_vec, d_vec, e_vec, k_b, block, hash_orig);
    Loop_round_0_61_body(3, k_b);
    Loop_rounds_0_63_body(4, v4, e_vec, f_vec, g_vec, h_vec, a_vec, b_vec, c_vec, d_vec, k_b, block, hash_orig);
    Loop_round_0_61_body(4, k_b);
    Loop_rounds_1_15_shift_body(5, v5, v4, block);
    Loop_rounds_0_63_body(5, v5, d_vec, e_vec, f_vec, g_vec, h_vec, a_vec, b_vec, c_vec, k_b, block, hash_orig);
    Loop_round_0_61_body(5, k_b);
    Loop_rounds_1_15_shift_body(6, v6, v5, block);
    Loop_rounds_0_63_body(6, v6, c_vec, d_vec, e_vec, f_vec, g_vec, h_vec, a_vec, b_vec, k_b, block, hash_orig);
    Loop_round_0_61_body(6, k_b);

    Loop_rounds_3_7_11_body(7, v8, in_b, offset+2);
    assert v8 == index(input_BE, 2);
    Loop_rounds_1_15_shift_body(7, v7, v6, block);
    Loop_rounds_0_63_body(7, v7, b_vec, c_vec, d_vec, e_vec, f_vec, g_vec, h_vec, a_vec, k_b, block, hash_orig);
    Loop_round_0_61_body(7, k_b);
    Loop_rounds_0_63_body(8, v8, a_vec, b_vec, c_vec, d_vec, e_vec, f_vec, g_vec, h_vec, k_b, block, hash_orig);
    Loop_round_0_61_body(8, k_b);
    Loop_rounds_1_15_shift_body(9, v9, v8, block);
    Loop_rounds_0_63_body(9, v9, h_vec, a_vec, b_vec, c_vec, d_vec, e_vec, f_vec, g_vec, k_b, block, hash_orig);
    Loop_round_0_61_body(9, k_b);
    Loop_rounds_1_15_shift_body(10, v10, v9, block);
    Loop_rounds_0_63_body(10, v10, g_vec, h_vec, a_vec, b_vec, c_vec, d_vec, e_vec, f_vec, k_b, block, hash_orig);
    Loop_round_0_61_body(10, k_b);

    Loop_rounds_3_7_11_body(11, v12, in_b, offset+3);
    assert v12 == index(input_BE, 3);
    Loop_rounds_1_15_shift_body(11, v11, v10, block);
    Loop_rounds_0_63_body(11, v11, f_vec, g_vec, h_vec, a_vec, b_vec, c_vec, d_vec, e_vec, k_b, block, hash_orig);
    Loop_round_0_61_body(11, k_b);
    Loop_rounds_0_63_body(12, v12, e_vec, f_vec, g_vec, h_vec, a_vec, b_vec, c_vec, d_vec, k_b, block, hash_orig);
    Loop_round_0_61_body(12, k_b);
    Loop_rounds_1_15_shift_body(13, v13, v12, block);
    Loop_rounds_0_63_body(13, v13, d_vec, e_vec, f_vec, g_vec, h_vec, a_vec, b_vec, c_vec, k_b, block, hash_orig);
    Loop_round_0_61_body(13, k_b);
    Loop_rounds_1_15_shift_body(14, v14, v13, block);
    Loop_rounds_0_63_body(14, v14, c_vec, d_vec, e_vec, f_vec, g_vec, h_vec, a_vec, b_vec, k_b, block, hash_orig);
    Loop_round_0_61_body(14, k_b);
    Loop_rounds_1_15_shift_body(15, v15, v14, block);
    Loop_rounds_16_63_body(16, v0, v1, v9, v14, block);
    Loop_rounds_0_63_body(15, v15, b_vec, c_vec, d_vec, e_vec, f_vec, g_vec, h_vec, a_vec, k_b, block, hash_orig);
    Loop_round_0_61_body(15, k_b);
}

procedure Loop_rounds_16_31(
        ghost k_b:buffer128,
        ghost block:block_w,
        ghost hash_orig:hash256)
    {:quick}
    lets
        tbl @= r6;
        a_vec @= v16; b_vec @= v17; c_vec @= v18; d_vec @= v19;
        e_vec @= v20; f_vec @= v21; g_vec @= v22; h_vec @= v23;
        Wi @= v24; tmp_vec @= v25; tmp_vec2 @= v26;
    reads
        heap0; memLayout;
    modifies
        tbl; r10; v0; v1; v2; v3; v4; v5; v6; v7; v8; v9; v10; v11; v12; v13; v14; v15; a_vec; b_vec; c_vec; d_vec; e_vec; f_vec; g_vec; h_vec; Wi; tmp_vec; tmp_vec2;
    requires
        validSrcAddrsOffset128(heap0, tbl, k_b, 5, 4, memLayout, Secret);
        let ks := buffer128_as_seq(heap0, k_b);
        k_reqs(ks);
        tbl + 64 < pow2_64;
        let hash := #(seq(Vale.SHA.SHA_helpers.word))(repeat_range_vale(16, block, hash_orig)) in 
        a_vec.hi3 == word_to_nat32(index(hash, 0)) /\
        b_vec.hi3 == word_to_nat32(index(hash, 1)) /\
        c_vec.hi3 == word_to_nat32(index(hash, 2)) /\
        d_vec.hi3 == word_to_nat32(index(hash, 3)) /\
        e_vec.hi3 == word_to_nat32(index(hash, 4)) /\
        f_vec.hi3 == word_to_nat32(index(hash, 5)) /\
        g_vec.hi3 == word_to_nat32(index(hash, 6)) /\
        h_vec.hi3 == add_wrap32(word_to_nat32(index(hash, 7)), k_index(ks, 16));
        v0.hi3 == ws_opaque(block, 16) /\ v1.hi3 == ws_opaque(block, 1) /\ v2.hi3 == ws_opaque(block, 2) /\
        v3.hi3 == ws_opaque(block, 3) /\ v4.hi3 == ws_opaque(block, 4) /\ v5.hi3 == ws_opaque(block, 5) /\
        v6.hi3 == ws_opaque(block, 6) /\ v7.hi3 == ws_opaque(block, 7) /\ v8.hi3 == ws_opaque(block, 8) /\
        v9.hi3 == ws_opaque(block, 9) /\ v10.hi3 == ws_opaque(block, 10) /\ v11.hi3 == ws_opaque(block, 11) /\
        v12.hi3 == ws_opaque(block, 12) /\ v13.hi3 == ws_opaque(block, 13) /\ v14.hi3 == ws_opaque(block, 14) /\
        v15.hi3 == ws_opaque(block, 15);
        Wi.hi3 == k_index(ks, 17) && Wi.hi2 == k_index(ks, 18) && Wi.lo1 == k_index(ks, 19);
    ensures
        tbl == old(tbl) + 64;
        let ks := buffer128_as_seq(heap0, k_b);
        let next_hash := #(seq(Vale.SHA.SHA_helpers.word))(repeat_range_vale(32, block, hash_orig)) in 
        a_vec.hi3 == word_to_nat32(index(next_hash, 0)) /\
        b_vec.hi3 == word_to_nat32(index(next_hash, 1)) /\
        c_vec.hi3 == word_to_nat32(index(next_hash, 2)) /\
        d_vec.hi3 == word_to_nat32(index(next_hash, 3)) /\
        e_vec.hi3 == word_to_nat32(index(next_hash, 4)) /\
        f_vec.hi3 == word_to_nat32(index(next_hash, 5)) /\
        g_vec.hi3 == word_to_nat32(index(next_hash, 6)) /\
        h_vec.hi3 == add_wrap32(word_to_nat32(index(next_hash, 7)), k_index(ks, 32));
        v0.hi3 == ws_opaque(block, 32) /\ v1.hi3 == ws_opaque(block, 17) /\ v2.hi3 == ws_opaque(block, 18) /\
        v3.hi3 == ws_opaque(block, 19) /\ v4.hi3 == ws_opaque(block, 20) /\ v5.hi3 == ws_opaque(block, 21) /\
        v6.hi3 == ws_opaque(block, 22) /\ v7.hi3 == ws_opaque(block, 23) /\ v8.hi3 == ws_opaque(block, 24) /\
        v9.hi3 == ws_opaque(block, 25) /\ v10.hi3 == ws_opaque(block, 26) /\ v11.hi3 == ws_opaque(block, 27) /\
        v12.hi3 == ws_opaque(block, 28) /\ v13.hi3 == ws_opaque(block, 29) /\ v14.hi3 == ws_opaque(block, 30) /\
        v15.hi3 == ws_opaque(block, 31);
        Wi.hi3 == k_index(ks, 33) && Wi.hi2 == k_index(ks, 34) && Wi.lo1 == k_index(ks, 35);
{
    Loop_rounds_16_63_body(17, v1, v2, v10, v15, block);
    Loop_rounds_0_63_body(16, v0, a_vec, b_vec, c_vec, d_vec, e_vec, f_vec, g_vec, h_vec, k_b, block, hash_orig);
    Loop_round_0_61_body(16, k_b);
    Loop_rounds_16_63_body(18, v2, v3, v11, v0, block);
    Loop_rounds_0_63_body(17, v1, h_vec, a_vec, b_vec, c_vec, d_vec, e_vec, f_vec, g_vec, k_b, block, hash_orig);
    Loop_round_0_61_body(17, k_b);
    Loop_rounds_16_63_body(19, v3, v4, v12, v1, block);
    Loop_rounds_0_63_body(18, v2, g_vec, h_vec, a_vec, b_vec, c_vec, d_vec, e_vec, f_vec, k_b, block, hash_orig);
    Loop_round_0_61_body(18, k_b);
    Loop_rounds_16_63_body(20, v4, v5, v13, v2, block);
    Loop_rounds_0_63_body(19, v3, f_vec, g_vec, h_vec, a_vec, b_vec, c_vec, d_vec, e_vec, k_b, block, hash_orig);
    Loop_round_0_61_body(19, k_b);

    Loop_rounds_16_63_body(21, v5, v6, v14, v3, block);
    Loop_rounds_0_63_body(20, v4, e_vec, f_vec, g_vec, h_vec, a_vec, b_vec, c_vec, d_vec, k_b, block, hash_orig);
    Loop_round_0_61_body(20, k_b);
    Loop_rounds_16_63_body(22, v6, v7, v15, v4, block);
    Loop_rounds_0_63_body(21, v5, d_vec, e_vec, f_vec, g_vec, h_vec, a_vec, b_vec, c_vec, k_b, block, hash_orig);
    Loop_round_0_61_body(21, k_b);
    Loop_rounds_16_63_body(23, v7, v8, v0, v5, block);
    Loop_rounds_0_63_body(22, v6, c_vec, d_vec, e_vec, f_vec, g_vec, h_vec, a_vec, b_vec, k_b, block, hash_orig);
    Loop_round_0_61_body(22, k_b);
    Loop_rounds_16_63_body(24, v8, v9, v1, v6, block);
    Loop_rounds_0_63_body(23, v7, b_vec, c_vec, d_vec, e_vec, f_vec, g_vec, h_vec, a_vec, k_b, block, hash_orig);
    Loop_round_0_61_body(23, k_b);

    Loop_rounds_16_63_body(25, v9, v10, v2, v7, block);
    Loop_rounds_0_63_body(24, v8, a_vec, b_vec, c_vec, d_vec, e_vec, f_vec, g_vec, h_vec, k_b, block, hash_orig);
    Loop_round_0_61_body(24, k_b);
    Loop_rounds_16_63_body(26, v10, v11, v3, v8, block);
    Loop_rounds_0_63_body(25, v9, h_vec, a_vec, b_vec, c_vec, d_vec, e_vec, f_vec, g_vec, k_b, block, hash_orig);
    Loop_round_0_61_body(25, k_b);
    Loop_rounds_16_63_body(27, v11, v12, v4, v9, block);
    Loop_rounds_0_63_body(26, v10, g_vec, h_vec, a_vec, b_vec, c_vec, d_vec, e_vec, f_vec, k_b, block, hash_orig);
    Loop_round_0_61_body(26, k_b);
    Loop_rounds_16_63_body(28, v12, v13, v5, v10, block);
    Loop_rounds_0_63_body(27, v11, f_vec, g_vec, h_vec, a_vec, b_vec, c_vec, d_vec, e_vec, k_b, block, hash_orig);
    Loop_round_0_61_body(27, k_b);

    Loop_rounds_16_63_body(29, v13, v14, v6, v11, block);
    Loop_rounds_0_63_body(28, v12, e_vec, f_vec, g_vec, h_vec, a_vec, b_vec, c_vec, d_vec, k_b, block, hash_orig);
    Loop_round_0_61_body(28, k_b);
    Loop_rounds_16_63_body(30, v14, v15, v7, v12, block);
    Loop_rounds_0_63_body(29, v13, d_vec, e_vec, f_vec, g_vec, h_vec, a_vec, b_vec, c_vec, k_b, block, hash_orig);
    Loop_round_0_61_body(29, k_b);
    Loop_rounds_16_63_body(31, v15, v0, v8, v13, block);
    Loop_rounds_0_63_body(30, v14, c_vec, d_vec, e_vec, f_vec, g_vec, h_vec, a_vec, b_vec, k_b, block, hash_orig);
    Loop_round_0_61_body(30, k_b);
    Loop_rounds_16_63_body(32, v0, v1, v9, v14, block);
    Loop_rounds_0_63_body(31, v15, b_vec, c_vec, d_vec, e_vec, f_vec, g_vec, h_vec, a_vec, k_b, block, hash_orig);
    Loop_round_0_61_body(31, k_b);
}

procedure Loop_rounds_32_47(
        ghost k_b:buffer128,
        ghost block:block_w,
        ghost hash_orig:hash256)
    {:quick}
    lets
        tbl @= r6;
        a_vec @= v16; b_vec @= v17; c_vec @= v18; d_vec @= v19;
        e_vec @= v20; f_vec @= v21; g_vec @= v22; h_vec @= v23;
        Wi @= v24; tmp_vec @= v25; tmp_vec2 @= v26;
    reads
        heap0; memLayout;
    modifies
        tbl; r10; v0; v1; v2; v3; v4; v5; v6; v7; v8; v9; v10; v11; v12; v13; v14; v15; a_vec; b_vec; c_vec; d_vec; e_vec; f_vec; g_vec; h_vec; Wi; tmp_vec; tmp_vec2;
    requires
        validSrcAddrsOffset128(heap0, tbl, k_b, 9, 4, memLayout, Secret);
        let ks := buffer128_as_seq(heap0, k_b);
        k_reqs(ks);
        tbl + 64 < pow2_64;
        let hash := #(seq(Vale.SHA.SHA_helpers.word))(repeat_range_vale(32, block, hash_orig)) in 
        a_vec.hi3 == word_to_nat32(index(hash, 0)) /\
        b_vec.hi3 == word_to_nat32(index(hash, 1)) /\
        c_vec.hi3 == word_to_nat32(index(hash, 2)) /\
        d_vec.hi3 == word_to_nat32(index(hash, 3)) /\
        e_vec.hi3 == word_to_nat32(index(hash, 4)) /\
        f_vec.hi3 == word_to_nat32(index(hash, 5)) /\
        g_vec.hi3 == word_to_nat32(index(hash, 6)) /\
        h_vec.hi3 == add_wrap32(word_to_nat32(index(hash, 7)), k_index(ks, 32));
        v0.hi3 == ws_opaque(block, 32) /\ v1.hi3 == ws_opaque(block, 17) /\ v2.hi3 == ws_opaque(block, 18) /\
        v3.hi3 == ws_opaque(block, 19) /\ v4.hi3 == ws_opaque(block, 20) /\ v5.hi3 == ws_opaque(block, 21) /\
        v6.hi3 == ws_opaque(block, 22) /\ v7.hi3 == ws_opaque(block, 23) /\ v8.hi3 == ws_opaque(block, 24) /\
        v9.hi3 == ws_opaque(block, 25) /\ v10.hi3 == ws_opaque(block, 26) /\ v11.hi3 == ws_opaque(block, 27) /\
        v12.hi3 == ws_opaque(block, 28) /\ v13.hi3 == ws_opaque(block, 29) /\ v14.hi3 == ws_opaque(block, 30) /\
        v15.hi3 == ws_opaque(block, 31);
        Wi.hi3 == k_index(ks, 33) && Wi.hi2 == k_index(ks, 34) && Wi.lo1 == k_index(ks, 35);
    ensures
        tbl == old(tbl) + 64;
        let ks := buffer128_as_seq(heap0, k_b);
        let next_hash := #(seq(Vale.SHA.SHA_helpers.word))(repeat_range_vale(48, block, hash_orig)) in 
        a_vec.hi3 == word_to_nat32(index(next_hash, 0)) /\
        b_vec.hi3 == word_to_nat32(index(next_hash, 1)) /\
        c_vec.hi3 == word_to_nat32(index(next_hash, 2)) /\
        d_vec.hi3 == word_to_nat32(index(next_hash, 3)) /\
        e_vec.hi3 == word_to_nat32(index(next_hash, 4)) /\
        f_vec.hi3 == word_to_nat32(index(next_hash, 5)) /\
        g_vec.hi3 == word_to_nat32(index(next_hash, 6)) /\
        h_vec.hi3 == add_wrap32(word_to_nat32(index(next_hash, 7)), k_index(ks, 48));
        v0.hi3 == ws_opaque(block, 48) /\ v1.hi3 == ws_opaque(block, 33) /\ v2.hi3 == ws_opaque(block, 34) /\
        v3.hi3 == ws_opaque(block, 35) /\ v4.hi3 == ws_opaque(block, 36) /\ v5.hi3 == ws_opaque(block, 37) /\
        v6.hi3 == ws_opaque(block, 38) /\ v7.hi3 == ws_opaque(block, 39) /\ v8.hi3 == ws_opaque(block, 40) /\
        v9.hi3 == ws_opaque(block, 41) /\ v10.hi3 == ws_opaque(block, 42) /\ v11.hi3 == ws_opaque(block, 43) /\
        v12.hi3 == ws_opaque(block, 44) /\ v13.hi3 == ws_opaque(block, 45) /\ v14.hi3 == ws_opaque(block, 46) /\
        v15.hi3 == ws_opaque(block, 47);
        Wi.hi3 == k_index(ks, 49) && Wi.hi2 == k_index(ks, 50) && Wi.lo1 == k_index(ks, 51);
{
    Loop_rounds_16_63_body(33, v1, v2, v10, v15, block);
    Loop_rounds_0_63_body(32, v0, a_vec, b_vec, c_vec, d_vec, e_vec, f_vec, g_vec, h_vec, k_b, block, hash_orig);
    Loop_round_0_61_body(32, k_b);
    Loop_rounds_16_63_body(34, v2, v3, v11, v0, block);
    Loop_rounds_0_63_body(33, v1, h_vec, a_vec, b_vec, c_vec, d_vec, e_vec, f_vec, g_vec, k_b, block, hash_orig);
    Loop_round_0_61_body(33, k_b);
    Loop_rounds_16_63_body(35, v3, v4, v12, v1, block);
    Loop_rounds_0_63_body(34, v2, g_vec, h_vec, a_vec, b_vec, c_vec, d_vec, e_vec, f_vec, k_b, block, hash_orig);
    Loop_round_0_61_body(34, k_b);
    Loop_rounds_16_63_body(36, v4, v5, v13, v2, block);
    Loop_rounds_0_63_body(35, v3, f_vec, g_vec, h_vec, a_vec, b_vec, c_vec, d_vec, e_vec, k_b, block, hash_orig);
    Loop_round_0_61_body(35, k_b);

    Loop_rounds_16_63_body(37, v5, v6, v14, v3, block);
    Loop_rounds_0_63_body(36, v4, e_vec, f_vec, g_vec, h_vec, a_vec, b_vec, c_vec, d_vec, k_b, block, hash_orig);
    Loop_round_0_61_body(36, k_b);
    Loop_rounds_16_63_body(38, v6, v7, v15, v4, block);
    Loop_rounds_0_63_body(37, v5, d_vec, e_vec, f_vec, g_vec, h_vec, a_vec, b_vec, c_vec, k_b, block, hash_orig);
    Loop_round_0_61_body(37, k_b);
    Loop_rounds_16_63_body(39, v7, v8, v0, v5, block);
    Loop_rounds_0_63_body(38, v6, c_vec, d_vec, e_vec, f_vec, g_vec, h_vec, a_vec, b_vec, k_b, block, hash_orig);
    Loop_round_0_61_body(38, k_b);
    Loop_rounds_16_63_body(40, v8, v9, v1, v6, block);
    Loop_rounds_0_63_body(39, v7, b_vec, c_vec, d_vec, e_vec, f_vec, g_vec, h_vec, a_vec, k_b, block, hash_orig);
    Loop_round_0_61_body(39, k_b);

    Loop_rounds_16_63_body(41, v9, v10, v2, v7, block);
    Loop_rounds_0_63_body(40, v8, a_vec, b_vec, c_vec, d_vec, e_vec, f_vec, g_vec, h_vec, k_b, block, hash_orig);
    Loop_round_0_61_body(40, k_b);
    Loop_rounds_16_63_body(42, v10, v11, v3, v8, block);
    Loop_rounds_0_63_body(41, v9, h_vec, a_vec, b_vec, c_vec, d_vec, e_vec, f_vec, g_vec, k_b, block, hash_orig);
    Loop_round_0_61_body(41, k_b);
    Loop_rounds_16_63_body(43, v11, v12, v4, v9, block);
    Loop_rounds_0_63_body(42, v10, g_vec, h_vec, a_vec, b_vec, c_vec, d_vec, e_vec, f_vec, k_b, block, hash_orig);
    Loop_round_0_61_body(42, k_b);
    Loop_rounds_16_63_body(44, v12, v13, v5, v10, block);
    Loop_rounds_0_63_body(43, v11, f_vec, g_vec, h_vec, a_vec, b_vec, c_vec, d_vec, e_vec, k_b, block, hash_orig);
    Loop_round_0_61_body(43, k_b);

    Loop_rounds_16_63_body(45, v13, v14, v6, v11, block);
    Loop_rounds_0_63_body(44, v12, e_vec, f_vec, g_vec, h_vec, a_vec, b_vec, c_vec, d_vec, k_b, block, hash_orig);
    Loop_round_0_61_body(44, k_b);
    Loop_rounds_16_63_body(46, v14, v15, v7, v12, block);
    Loop_rounds_0_63_body(45, v13, d_vec, e_vec, f_vec, g_vec, h_vec, a_vec, b_vec, c_vec, k_b, block, hash_orig);
    Loop_round_0_61_body(45, k_b);
    Loop_rounds_16_63_body(47, v15, v0, v8, v13, block);
    Loop_rounds_0_63_body(46, v14, c_vec, d_vec, e_vec, f_vec, g_vec, h_vec, a_vec, b_vec, k_b, block, hash_orig);
    Loop_round_0_61_body(46, k_b);
    Loop_rounds_16_63_body(48, v0, v1, v9, v14, block);
    Loop_rounds_0_63_body(47, v15, b_vec, c_vec, d_vec, e_vec, f_vec, g_vec, h_vec, a_vec, k_b, block, hash_orig);
    Loop_round_0_61_body(47, k_b);
}

procedure Loop_rounds_48_63(
        ghost k_b:buffer128,
        ghost block:block_w,
        ghost hash_orig:hash256)
    {:quick}
    lets
        tbl @= r6;
        a_vec @= v16; b_vec @= v17; c_vec @= v18; d_vec @= v19;
        e_vec @= v20; f_vec @= v21; g_vec @= v22; h_vec @= v23;
        Wi @= v24; tmp_vec @= v25; tmp_vec2 @= v26;
    reads
        heap0; memLayout;
    modifies
        tbl; r10; v0; v1; v2; v3; v4; v5; v6; v7; v8; v9; v10; v11; v12; v13; v14; v15; a_vec; b_vec; c_vec; d_vec; e_vec; f_vec; g_vec; h_vec; Wi; tmp_vec; tmp_vec2;
    requires
        validSrcAddrsOffset128(heap0, tbl, k_b, 13, 3, memLayout, Secret);
        let ks := buffer128_as_seq(heap0, k_b);
        k_reqs(ks);
        tbl + 48 < pow2_64;
        let hash := #(seq(Vale.SHA.SHA_helpers.word))(repeat_range_vale(48, block, hash_orig)) in 
        a_vec.hi3 == word_to_nat32(index(hash, 0)) /\
        b_vec.hi3 == word_to_nat32(index(hash, 1)) /\
        c_vec.hi3 == word_to_nat32(index(hash, 2)) /\
        d_vec.hi3 == word_to_nat32(index(hash, 3)) /\
        e_vec.hi3 == word_to_nat32(index(hash, 4)) /\
        f_vec.hi3 == word_to_nat32(index(hash, 5)) /\
        g_vec.hi3 == word_to_nat32(index(hash, 6)) /\
        h_vec.hi3 == add_wrap32(word_to_nat32(index(hash, 7)), k_index(ks, 48));
        v0.hi3 == ws_opaque(block, 48) /\ v1.hi3 == ws_opaque(block, 33) /\ v2.hi3 == ws_opaque(block, 34) /\
        v3.hi3 == ws_opaque(block, 35) /\ v4.hi3 == ws_opaque(block, 36) /\ v5.hi3 == ws_opaque(block, 37) /\
        v6.hi3 == ws_opaque(block, 38) /\ v7.hi3 == ws_opaque(block, 39) /\ v8.hi3 == ws_opaque(block, 40) /\
        v9.hi3 == ws_opaque(block, 41) /\ v10.hi3 == ws_opaque(block, 42) /\ v11.hi3 == ws_opaque(block, 43) /\
        v12.hi3 == ws_opaque(block, 44) /\ v13.hi3 == ws_opaque(block, 45) /\ v14.hi3 == ws_opaque(block, 46) /\
        v15.hi3 == ws_opaque(block, 47);
        Wi.hi3 == k_index(ks, 49) && Wi.hi2 == k_index(ks, 50) && Wi.lo1 == k_index(ks, 51);
    ensures
        tbl == old(tbl) + 48;
        make_seperated_hash_quad32(a_vec, b_vec, c_vec, d_vec, e_vec, f_vec, g_vec, h_vec) == repeat_range_vale_64(block, hash_orig);
{
    Loop_rounds_16_63_body(49, v1, v2, v10, v15, block);
    Loop_rounds_0_63_body(48, v0, a_vec, b_vec, c_vec, d_vec, e_vec, f_vec, g_vec, h_vec, k_b, block, hash_orig);
    Loop_round_0_61_body(48, k_b);
    Loop_rounds_16_63_body(50, v2, v3, v11, v0, block);
    Loop_rounds_0_63_body(49, v1, h_vec, a_vec, b_vec, c_vec, d_vec, e_vec, f_vec, g_vec, k_b, block, hash_orig);
    Loop_round_0_61_body(49, k_b);
    Loop_rounds_16_63_body(51, v3, v4, v12, v1, block);
    Loop_rounds_0_63_body(50, v2, g_vec, h_vec, a_vec, b_vec, c_vec, d_vec, e_vec, f_vec, k_b, block, hash_orig);
    Loop_round_0_61_body(50, k_b);
    Loop_rounds_16_63_body(52, v4, v5, v13, v2, block);
    Loop_rounds_0_63_body(51, v3, f_vec, g_vec, h_vec, a_vec, b_vec, c_vec, d_vec, e_vec, k_b, block, hash_orig);
    Loop_round_0_61_body(51, k_b);

    Loop_rounds_16_63_body(53, v5, v6, v14, v3, block);
    Loop_rounds_0_63_body(52, v4, e_vec, f_vec, g_vec, h_vec, a_vec, b_vec, c_vec, d_vec, k_b, block, hash_orig);
    Loop_round_0_61_body(52, k_b);
    Loop_rounds_16_63_body(54, v6, v7, v15, v4, block);
    Loop_rounds_0_63_body(53, v5, d_vec, e_vec, f_vec, g_vec, h_vec, a_vec, b_vec, c_vec, k_b, block, hash_orig);
    Loop_round_0_61_body(53, k_b);
    Loop_rounds_16_63_body(55, v7, v8, v0, v5, block);
    Loop_rounds_0_63_body(54, v6, c_vec, d_vec, e_vec, f_vec, g_vec, h_vec, a_vec, b_vec, k_b, block, hash_orig);
    Loop_round_0_61_body(54, k_b);
    Loop_rounds_16_63_body(56, v8, v9, v1, v6, block);
    Loop_rounds_0_63_body(55, v7, b_vec, c_vec, d_vec, e_vec, f_vec, g_vec, h_vec, a_vec, k_b, block, hash_orig);
    Loop_round_0_61_body(55, k_b);

    Loop_rounds_16_63_body(57, v9, v10, v2, v7, block);
    Loop_rounds_0_63_body(56, v8, a_vec, b_vec, c_vec, d_vec, e_vec, f_vec, g_vec, h_vec, k_b, block, hash_orig);
    Loop_round_0_61_body(56, k_b);
    Loop_rounds_16_63_body(58, v10, v11, v3, v8, block);
    Loop_rounds_0_63_body(57, v9, h_vec, a_vec, b_vec, c_vec, d_vec, e_vec, f_vec, g_vec, k_b, block, hash_orig);
    Loop_round_0_61_body(57, k_b);
    Loop_rounds_16_63_body(59, v11, v12, v4, v9, block);
    Loop_rounds_0_63_body(58, v10, g_vec, h_vec, a_vec, b_vec, c_vec, d_vec, e_vec, f_vec, k_b, block, hash_orig);
    Loop_round_0_61_body(58, k_b);
    Loop_rounds_16_63_body(60, v12, v13, v5, v10, block);
    Loop_rounds_0_63_body(59, v11, f_vec, g_vec, h_vec, a_vec, b_vec, c_vec, d_vec, e_vec, k_b, block, hash_orig);
    Loop_round_0_61_body(59, k_b);

    Loop_rounds_16_63_body(61, v13, v14, v6, v11, block);
    Loop_rounds_0_63_body(60, v12, e_vec, f_vec, g_vec, h_vec, a_vec, b_vec, c_vec, d_vec, k_b, block, hash_orig);
    Loop_round_0_61_body(60, k_b);
    Loop_rounds_16_63_body(62, v14, v15, v7, v12, block);
    Loop_rounds_0_63_body(61, v13, d_vec, e_vec, f_vec, g_vec, h_vec, a_vec, b_vec, c_vec, k_b, block, hash_orig);
    Loop_round_0_61_body(61, k_b);
    Loop_rounds_16_63_body(63, v15, v0, v8, v13, block);
    Loop_rounds_0_63_body(62, v14, c_vec, d_vec, e_vec, f_vec, g_vec, h_vec, a_vec, b_vec, k_b, block, hash_orig);
    Loop_rounds_0_63_body(63, v15, b_vec, c_vec, d_vec, e_vec, f_vec, g_vec, h_vec, a_vec, k_b, block, hash_orig);
}

procedure Loop_rounds(
        ghost in_b:buffer128,
        ghost k_b:buffer128,
        ghost offset:nat)
    {:quick}
    lets
        inp @= r4; tbl @= r6;
        a_vec @= v16; b_vec @= v17; c_vec @= v18; d_vec @= v19;
        e_vec @= v20; f_vec @= v21; g_vec @= v22; h_vec @= v23;
        Wi @= v24; tmp_vec @= v25; tmp_vec2 @= v26;
    reads
        heap0; memLayout;
    modifies
        inp; tbl; r10; v0; v1; v2; v3; v4; v5; v6; v7; v8; v9; v10; v11; v12; v13; v14; v15; a_vec; b_vec; c_vec; d_vec; e_vec; f_vec; g_vec; h_vec; Wi; tmp_vec; tmp_vec2; v28; v29; v30; v31;
    requires
        validSrcAddrsOffset128(heap0, inp, in_b, offset, 4, memLayout, Secret);
        validSrcAddrs128(heap0, tbl,    k_b, 16, memLayout, Secret);
        k_reqs(buffer128_as_seq(heap0, k_b));
        inp + 64 < pow2_64;
        tbl + 256 < pow2_64;
    ensures
        tbl == old(tbl);
        inp == old(inp) + 64;
        let block:block_w := quads_to_block_be(reverse_bytes_quad32_seq(slice(buffer128_as_seq(heap0, in_b), offset, offset+4)));
        let hash_orig := make_seperated_hash_quad32(old(a_vec), old(b_vec), old(c_vec), old(d_vec), old(e_vec), old(f_vec), old(g_vec), old(h_vec));
        let initial_hash := #(seq(Vale.SHA.SHA_helpers.word))(repeat_range_vale(0, block, hash_orig)) in
        let last_hash := #(seq(Vale.SHA.SHA_helpers.word))(repeat_range_vale_64(block, hash_orig)) in
        a_vec.hi3 == add_wrap32(word_to_nat32(index(last_hash, 0)), word_to_nat32(index(initial_hash, 0))) /\
        b_vec.hi3 == add_wrap32(word_to_nat32(index(last_hash, 1)), word_to_nat32(index(initial_hash, 1))) /\
        c_vec.hi3 == add_wrap32(word_to_nat32(index(last_hash, 2)), word_to_nat32(index(initial_hash, 2))) /\
        d_vec.hi3 == add_wrap32(word_to_nat32(index(last_hash, 3)), word_to_nat32(index(initial_hash, 3))) /\
        e_vec.hi3 == add_wrap32(word_to_nat32(index(last_hash, 4)), word_to_nat32(index(initial_hash, 4))) /\
        f_vec.hi3 == add_wrap32(word_to_nat32(index(last_hash, 5)), word_to_nat32(index(initial_hash, 5))) /\
        g_vec.hi3 == add_wrap32(word_to_nat32(index(last_hash, 6)), word_to_nat32(index(initial_hash, 6))) /\
        h_vec.hi3 == add_wrap32(word_to_nat32(index(last_hash, 7)), word_to_nat32(index(initial_hash, 7)));
{
    let input_LE := slice(buffer128_as_seq(heap0, in_b), offset, offset+4);
    let input_BE := reverse_bytes_quad32_seq(input_LE);
    let block:block_w := quads_to_block_be(input_BE);

    let hash_orig := make_seperated_hash_quad32(a_vec, b_vec, c_vec, d_vec, e_vec, f_vec, g_vec, h_vec);
    lemma_make_seperated_hash_quad32(old(a_vec), old(b_vec), old(c_vec), old(d_vec), old(e_vec), old(f_vec), old(g_vec), old(h_vec));

    Xxmrghd(v28, a_vec, e_vec);
    Xxmrghd(v29, b_vec, f_vec);
    Xxmrghd(v30, c_vec, g_vec);
    Xxmrghd(v31, d_vec, h_vec);
    
    Loop_prologue(in_b, offset, k_b, block, hash_orig);
    assert v0 == index(input_BE, 0);
    
    Loop_rounds_0_15(in_b, offset, k_b, block, hash_orig, input_BE);
    Loop_rounds_16_31(k_b, block, hash_orig);
    Loop_rounds_32_47(k_b, block, hash_orig);
    Loop_rounds_48_63(k_b, block, hash_orig);
    lemma_make_seperated_hash_quad32(a_vec, b_vec, c_vec, d_vec, e_vec, f_vec, g_vec, h_vec);
    SubImm(tbl, tbl, 256);

    Vsldoi(v0, v28, v28, 8);
    Vsldoi(v1, v29, v29, 8);
    Vsldoi(v2, v30, v30, 8);
    Vsldoi(v3, v31, v31, 8);
    Vadduwm(a_vec, a_vec, v28);
    Vadduwm(b_vec, b_vec, v29);
    Vadduwm(c_vec, c_vec, v30);
    Vadduwm(d_vec, d_vec, v31);
    Vadduwm(e_vec, e_vec, v0);
    Vadduwm(f_vec, f_vec, v1);
    Vadduwm(g_vec, g_vec, v2);
    Vadduwm(h_vec, h_vec, v3);
}

procedure Mod_cr0()
    {:quick}
    modifies
        cr0;
{}

procedure Loop(ghost in_b:buffer128, ghost k_b:buffer128)
    {:quick}
    lets
        inp @= r4; num @= r5; tbl @= r6;
    reads
        heap0; memLayout;
    modifies
        inp; tbl; num; r10; cr0;
        v0; v1; v2; v3; v4; v5; v6; v7; v8; v9; v10; v11; v12; v13; v14; v15; v16; v17; v18; v19; v20; v21; v22; v23; v24; v25; v26; v28; v29; v30; v31;
    requires
        validSrcAddrs128(heap0, inp,   in_b,  4*num, memLayout, Secret);
        validSrcAddrs128(heap0, tbl,    k_b, 16, memLayout, Secret);
        inp + 0x40*num < pow2_64;
        tbl + 256 < pow2_64;
        k_reqs(buffer128_as_seq(heap0, k_b));
    ensures
        inp == old(inp) + 0x40*old(num);
        num == 0;
{
    ghost var count:nat := 0;
    while (num > 0)
        invariant
            validSrcAddrs128(heap0, old(inp),   in_b,  4*old(num), memLayout, Secret);
            validSrcAddrs128(heap0,      tbl,    k_b,          16, memLayout, Secret);

            old(inp) + 0x40*old(num) < pow2_64;
            tbl + 256 < pow2_64;

            num == old(num) - count;
            inp == old(inp) + 0x40 * count;

            k_reqs(buffer128_as_seq(heap0, k_b));
        decreases num;
    {
        Mod_cr0();
        Loop_rounds(in_b, k_b, 4*count);
        SubImm(num, num, 1);
        count := count + 1;
    }
}

procedure Preamble(ghost ctx_b:buffer128)
    {:quick}
    lets
        ctx @= r3;
        a_vec @= v16; b_vec @= v17; c_vec @= v18; d_vec @= v19;
        e_vec @= v20; f_vec @= v21; g_vec @= v22; h_vec @= v23;
    reads
        ctx; memLayout; heap0;
    modifies
        r9; r10; a_vec; b_vec; c_vec; d_vec; e_vec; f_vec; g_vec; h_vec;
    requires
        validSrcAddrs128(heap0, ctx,  ctx_b,  2, memLayout, Secret);
    ensures
        let dcba := buffer128_read(ctx_b, 0, heap0) in
        let hgfe := buffer128_read(ctx_b, 1, heap0) in
        a_vec.hi3 == dcba.lo0 /\
        b_vec.hi3 == dcba.lo1 /\
        c_vec.hi3 == dcba.hi2 /\
        d_vec.hi3 == dcba.hi3 /\
        e_vec.hi3 == hgfe.lo0 /\
        f_vec.hi3 == hgfe.lo1 /\
        g_vec.hi3 == hgfe.hi2 /\
        h_vec.hi3 == hgfe.hi3 /\
        make_seperated_hash_quad32(a_vec, b_vec, c_vec, d_vec, e_vec, f_vec, g_vec, h_vec) == make_ordered_hash(dcba, hgfe);
{
    let dcba := buffer128_read(ctx_b, 0, heap0);
    let hgfe := buffer128_read(ctx_b, 1, heap0);
    let a := dcba.lo0;
    let b := dcba.lo1;
    let c := dcba.hi2;
    let d := dcba.hi3;
    let e := hgfe.lo0;
    let f := hgfe.lo1;
    let g := hgfe.hi2;
    let h := hgfe.hi3;
    LoadImm64(r9, 0);
    LoadImm64(r10, 16);
    Load128_word4_buffer(heap0, a_vec, ctx,  r9, Secret, ctx_b, 0);
    Load128_word4_buffer(heap0, e_vec, ctx, r10, Secret, ctx_b, 1);

    Vsldoi(b_vec, a_vec, a_vec, 4);
    Vsldoi(c_vec, a_vec, a_vec, 8);
    Vsldoi(d_vec, a_vec, a_vec, 12);
    Vsldoi(f_vec, e_vec, e_vec, 4);
    Vsldoi(g_vec, e_vec, e_vec, 8);
    Vsldoi(h_vec, e_vec, e_vec, 12);

    assert equal(#(seq(Vale.SHA.SHA_helpers.word))(make_seperated_hash(a, b, c, d, e, f, g, h)), #(seq(Vale.SHA.SHA_helpers.word))(make_ordered_hash(dcba, hgfe)));
    assert equal(#(seq(Vale.SHA.SHA_helpers.word))(make_seperated_hash(a, b, c, d, e, f, g, h)), #(seq(Vale.SHA.SHA_helpers.word))(make_seperated_hash_quad32(a_vec, b_vec, c_vec, d_vec, e_vec, f_vec, g_vec, h_vec)));
}

procedure Epilogue(ghost ctx_b:buffer128)
    {:quick}
    lets
        ctx @= r3;
        a_vec @= v16; b_vec @= v17; c_vec @= v18; d_vec @= v19;
        e_vec @= v20; f_vec @= v21; g_vec @= v22; h_vec @= v23;
    reads
        ctx; b_vec; d_vec; f_vec; h_vec; memLayout;
    modifies
        r9; r10; a_vec; c_vec; e_vec; g_vec; heap0;
    requires
        validDstAddrs128(heap0, ctx,  ctx_b,  2, memLayout, Secret);
    ensures
        modifies_buffer128(ctx_b, old(heap0), heap0);
{
    Vmrghw(a_vec, a_vec, b_vec);
    Vmrghw(c_vec, c_vec, d_vec);
    Xxmrghd(a_vec, a_vec, c_vec);

    Vmrghw(e_vec, e_vec, f_vec);
    Vmrghw(g_vec, g_vec, h_vec);
    Xxmrghd(e_vec, e_vec, g_vec);

    LoadImm64(r9, 0);
    LoadImm64(r10, 16);
    Store128_word4_buffer(heap0, a_vec, ctx,  r9, Secret, ctx_b, 0);
    Store128_word4_buffer(heap0, e_vec, ctx, r10, Secret, ctx_b, 1);
}

procedure Sha_update(
        ghost ctx_b:buffer128,
        ghost in_b:buffer128,
        ghost num_val:nat64,
        ghost k_b:buffer128)
    {:public}
    {:quick}
    {:exportSpecs}
    lets
        ctx @= r3; inp @= r4; num @= r5; tbl @= r6;
    reads
        ctx;
    modifies
        r1; inp; num; tbl; r9; r10; cr0;
        v0; v1; v2; v3; v4; v5; v6; v7; v8; v9; v10; v11; v12; v13; v14; v15; v16; v17; v18; v19; v20; v21; v22; v23; v24; v25; v26; v28; v29; v30; v31;
        heap0; memLayout; stack; stackTaint;
    requires
        locs_disjoint(list(loc_buffer(ctx_b), loc_buffer(in_b))) \/ ctx_b == in_b;
        locs_disjoint(list(loc_buffer(ctx_b), loc_buffer(k_b))) \/ ctx_b == k_b;
        locs_disjoint(list(loc_buffer(in_b), loc_buffer(k_b))) \/ in_b == k_b;
        validDstAddrs128(mem, ctx,  ctx_b,  2, memLayout, Secret);
        validSrcAddrs128(mem, inp,   in_b,  4*num, memLayout, Secret);
        validSrcAddrs128(mem, tbl,    k_b, 16, memLayout, Secret);
        num_val == num;
        r1 == init_r1(stack);
        is_initial_heap(memLayout, mem);
        inp + 0x40*num < pow2_64;
        tbl + 256 < pow2_64;
        buffers_disjoint128(ctx_b, in_b);
        buffer_length(ctx_b) == 2;
        buffer_length(in_b) == 4 * num;
        k_reqs(buffer128_as_seq(mem, k_b));
    ensures
        modifies_mem(loc_buffer(ctx_b), old(mem), mem);
        r1 == old(r1);
        v20 == old(v20) /\ v21 == old(v21) /\ v22 == old(v22) /\
        v23 == old(v23) /\ v24 == old(v24) /\ v25 == old(v25) /\
        v26 == old(v26) /\ v28 == old(v28) /\ v29 == old(v29) /\
        v30 == old(v30) /\ v31 == old(v31);
{
    CreateHeaplets(list(
        declare_buffer128(in_b, 0, Secret, Immutable),
        declare_buffer128(k_b, 0, Secret, Immutable),
        declare_buffer128(ctx_b, 0, Secret, Mutable)));

    Alloc_stack(16*11);
    Store_stack128(v20, 16*0);
    Store_stack128(v21, 16*1);
    Store_stack128(v22, 16*2);
    Store_stack128(v23, 16*3);
    Store_stack128(v24, 16*4);
    Store_stack128(v25, 16*5);
    Store_stack128(v26, 16*6);
    Store_stack128(v28, 16*7);
    Store_stack128(v29, 16*8);
    Store_stack128(v30, 16*9);
    Store_stack128(v31, 16*10);
    Preamble(ctx_b);
    Loop(in_b, k_b);
    Epilogue(ctx_b);
    Load_stack128(v20, 16*0);
    Load_stack128(v21, 16*1);
    Load_stack128(v22, 16*2);
    Load_stack128(v23, 16*3);
    Load_stack128(v24, 16*4);
    Load_stack128(v25, 16*5);
    Load_stack128(v26, 16*6);
    Load_stack128(v28, 16*7);
    Load_stack128(v29, 16*8);
    Load_stack128(v30, 16*9);
    Load_stack128(v31, 16*10);
    Dealloc_stack(16*11);

    DestroyHeaplets();
}
